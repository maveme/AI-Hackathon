{"cells":[{"cell_type":"markdown","id":"d7225862","metadata":{"id":"d7225862"},"source":["# <h1><center> **Hackathon AI - VU Amsterdam** </center></h1>\n","<h2><center>Physiological sensor data </center></h2>"]},{"cell_type":"markdown","id":"69903a28","metadata":{"id":"69903a28","outputId":"0d15928e-f6b7-4592-b5fe-6f89d3e28f65"},"source":["<br>\n","<center><img src=\"Sensor.png\" alt=\"\" width=\"700\"/></center>\n","<br>"]},{"cell_type":"markdown","id":"ba3fc021","metadata":{"id":"ba3fc021"},"source":["In this working group, your group are going to explore the ways and methods one can extract sensible knowledge from sensory data. Also, you will acquire some first insight into modern machine learning algorithms. The goal is (just as overall with the Hackathon) to have fun, but also to learn new AI skills! So let's get started!\n","\n","In the first step, you should download the dataset. The dataset can be downloaded [here](URL einfuegen). \n"]},{"cell_type":"markdown","id":"760591f9","metadata":{"id":"760591f9"},"source":["## Train and test split"]},{"cell_type":"markdown","id":"667c42c9","metadata":{"id":"667c42c9"},"source":["Normally, you should split your dataset into a so-called training dataset and a test set. Yet, in this workshop you will work with a dataset and generate your own test set! That is, you will first try to come up with a 'good'model and then collect your data and then see and check the performance of the algorithm again!"]},{"cell_type":"markdown","id":"cdc59dbb","metadata":{"id":"cdc59dbb"},"source":["## Getting the app for gathering your own data"]},{"cell_type":"markdown","id":"549bf491","metadata":{"id":"549bf491"},"source":["The app that we will use to gather your own data is 'Sensor Logger' developed by Dr. Kelvin Choi. The documentation as well as the links to the playstore can be found [here](https://www.tszheichoi.com/sensorlogger)"]},{"cell_type":"markdown","id":"5ab660f5","metadata":{"id":"5ab660f5"},"source":["Some people can get acquainted with the app and also start to collect the test data. The more data you have the better! Also, make sure to collect data for the following features:\n","\n","\n","**Accelerometer**:\n","The acceleration of the device in the x,y,z axis (so basically acceleration in space). \n","\n","**Gyroscope**:\n","The angular speed of the device, again in the x,y,z axis.\n","\n","**Magnetometer**:\n","The magnetometer value of the device.\n","\n","**Light lux feature**"]},{"cell_type":"markdown","id":"4ff35611","metadata":{"id":"4ff35611"},"source":["### Basic Terminologies"]},{"cell_type":"markdown","id":"6ec6aa1f","metadata":{"id":"6ec6aa1f"},"source":["Before we start with exploring the dataset, you should get familiar with a few basic terminologies. These definitions were taken from the book '“Machine Learning for the Quantified Self – On the Art of Learning from Sensory Data” (2018) by Hoogendoorn and Funk. \n","\n","**Machine learning**:\n","\n","Machine learning is the task to automatically identify patterns from the data.\n","\n","In machine learning, the difference between *supervised learning* and *unsupervised learning* is very important. I am sure that you have already heard these two terms, but here is a quick refresher:\n","\n","**Supervised learning**:\n","\n","Supervised learning relates to the task of inferring a function from labelled data. For example, one wants to predict the blood glucose level based on the food a person has eaten. \n","\n","**Unsupervised learning**:\n","\n","On the contrary to supervised learning, unsupervised learning wants to find patterns and associations among the attributes (variables, features). For example, one wants to find different clusters of customers based on their consumer behavior (past purchases etc.).\n","\n","In the following workshop, we will be mainly concerned with supervised learning. "]},{"cell_type":"markdown","id":"da2734a4","metadata":{"id":"da2734a4"},"source":["### Importing important libraries "]},{"cell_type":"code","execution_count":null,"id":"1543bfcf","metadata":{"executionInfo":{"elapsed":1707,"status":"ok","timestamp":1666699604363,"user":{"displayName":"B Ündes","userId":"13583163329665985789"},"user_tz":-120},"id":"1543bfcf"},"outputs":[],"source":["import pandas as pd\n","import io\n","import numpy as np\n","import datetime\n","import pandas as pd\n","import re\n","import copy\n","from pathlib import Path\n","import copy\n","import os\n","import sys\n","from datetime import datetime, timedelta\n","import matplotlib.pyplot as plot\n","import matplotlib.dates as md\n","import scipy.stats\n","import matplotlib.pyplot as plt\n","from sklearn.compose import ColumnTransformer\n","from sklearn.mixture import GaussianMixture\n","import seaborn as sns\n","from sklearn.pipeline import Pipeline\n","from statsmodels.graphics.gofplots import qqplot\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.linear_model import BayesianRidge\n","from sklearn.compose import make_column_transformer, ColumnTransformer\n","#from category_encoders import *\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","from sklearn.linear_model import BayesianRidge\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import ExtraTreesRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","#from category_encoders.target_encoder import TargetEncoder\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import mean_squared_error\n","from imblearn.over_sampling import SMOTE\n","import numpy as np\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.metrics import f1_score\n","#from prince import PCA\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from scipy.stats import skew \n","from sklearn.metrics import mean_squared_error, make_scorer \n","#import shap\n","from sklearn.model_selection import train_test_split\n","#from mlens.ensemble import SuperLearner, BlendEnsemble, Subsemble, SequentialEnsemble\n","from sklearn.cluster import KMeans\n","import os\n","from google.colab import files\n","from datetime import datetime, timedelta\n","from pytz import timezone\n","from sklearn.neighbors import LocalOutlierFactor\n","from sklearn.ensemble import AdaBoostClassifier\n","from scipy.stats import shapiro"]},{"cell_type":"markdown","id":"01b3dd35","metadata":{"id":"01b3dd35"},"source":["# Part 1: Preprocessing of the data"]},{"cell_type":"markdown","id":"ee8b16bc","metadata":{"id":"ee8b16bc"},"source":["As you can see, there are several data files available for you to use in the component workshop. The most important data files are the **Accelerometer, Gyroscope and the Magnetometer**. There are many more features (variables) that one could potentially use, but given the time constraint, we want to focus only on those simple ones.\n","\n","**Accelerometer**:\n","The acceleration of the device in the x,y,z axis (so basically acceleration in space). \n","\n","**Gyroscope**:\n","The angular speed of the device, again in the x,y,z axis.\n","\n","**Magnetometer**:\n","The magnetometer value of the device.\n","\n","**Light lux feature**\n","\n","Furthermore, we will use the light feature as well!\n","\n","It is important to check what your current working directory is\n","\n"]},{"cell_type":"code","execution_count":null,"id":"df06e6ab","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":170},"executionInfo":{"elapsed":7,"status":"error","timestamp":1666699371401,"user":{"displayName":"B Ündes","userId":"13583163329665985789"},"user_tz":-120},"id":"df06e6ab","outputId":"471da1a7-c152-4976-e80f-0779460a780d"},"outputs":[],"source":["os.getcwd()"]},{"cell_type":"markdown","id":"3a429208","metadata":{"id":"3a429208"},"source":["Sensor data usually come with a timestamp expressed in nano seconds. Hence, one needs to convert the timestamp into a format that is readable for us humans. Luckily, this part has been already done for you, so you can focus on more fun parts! Yet, below you can find the code that was used to convert the timestamps into human-readable formats."]},{"cell_type":"code","execution_count":null,"id":"d7da0e6b","metadata":{"id":"d7da0e6b"},"outputs":[],"source":["def format_my_nanos_features(nanos):\n","    dt = datetime.fromtimestamp(nanos['timestamps'] / 1e9)\n","    dt = dt.astimezone(timezone('US/Pacific'))\n","    print(dt)\n","    return '{}{:03.0f}'.format(dt.strftime('%Y/%m/%d %H:%M:%S.%f'), nanos['timestamps'] % 1e3)\n","\n","def format_my_nanos_label_start(nanos):\n","    dt = datetime.fromtimestamp(nanos['label_start']/ 1e9)\n","    dt = dt.astimezone(timezone('US/Pacific'))\n","    print(dt)\n","    return '{}{:03.0f}'.format(dt.strftime('%Y/%m/%d %H:%M:%S.%f'), nanos['label_start'] % 1e3)\n","\n","def format_my_nanos_label_end(nanos):\n","    dt = datetime.fromtimestamp(nanos['label_end']/ 1e9)\n","    dt = dt.astimezone(timezone('US/Pacific'))\n","    print(dt)\n","    return '{}{:03.0f}'.format(dt.strftime('%Y/%m/%d %H:%M:%S.%f'), nanos['label_end'] % 1e3)\n"]},{"cell_type":"markdown","id":"25b6a232","metadata":{"id":"25b6a232"},"source":["## Converting raw data to an aggregated data format "]},{"cell_type":"markdown","id":"1be61ee5","metadata":{"id":"1be61ee5"},"source":["An important aspect is the timestep of the data that we going to use. For instance, we could say that we want to measure the above-mentioned features 4 times per second, i.e. 250 nano seconds. Alternatively, we could say we would like to have measure only every half a minute. \n","\n","**Question** \n","\n","What are advantages and disadvantages of measuring at a higher frequency, i.e. 250 nano seconds?"]},{"cell_type":"markdown","id":"91fc7cdc","metadata":{"id":"91fc7cdc"},"source":["*Here you can write some thoughts*"]},{"cell_type":"markdown","id":"8650dd53","metadata":{"id":"8650dd53"},"source":["After we have decided on the specific measure frequency or also called **granularity**, we need to decide how we deal with all observations that fall into the time window. For istance, what if we have 10 observations within the decided time window of 30 seconds? How do we include the information of all those ten observations in one single observation?\n","\n","In practice, people use different methods based on the specific variable one considers. As we deal with numerical variables, i.e. it takes real values, most people simply take the mean. In the above-mentioned example, one would only use a simple average of all 10 observations. \n","\n","For the label data, things are different since we are dealing with a categorical variable. That is, the value of the variables are categories, 'sitting' or 'on the table'. For this, one simply creates a new variable which takes the value of 1 if a specific label is present during the time window, and 0 otherwise. Such a variable is called a **binary** variable as it takes only two values.\n","\n","As the preprocessing and aggregation takes a lot of time, and since time is restricted in this Hackathon, this part has already been done for you. Yet, it is important to know how your data has been preprocessed. \n","\n","The following datasets have a granularity of 250 nanoseconds. This is the granularity with which we will work with."]},{"cell_type":"markdown","id":"c2e3333e","metadata":{"id":"c2e3333e"},"source":["**Important** \n","\n","Do not change the code below!"]},{"cell_type":"code","execution_count":null,"id":"a454bf01","metadata":{"id":"a454bf01"},"outputs":[],"source":["class CreateDataset:\n","\n","    base_dir = ''\n","    granularity = 0\n","    data_table = None\n","    #file = files.upload()\n","\n","    def __init__(self, base_dir, granularity):\n","        self.base_dir = base_dir\n","        self.granularity = granularity    \n","    \n","    # Create an initial data table with entries from start till end time, with steps\n","    # of size granularity. Granularity is specified in milliseconds\n","    def create_timestamps(self, start_time, end_time):\n","        return pd.date_range(start_time, end_time, freq=str(self.granularity)+'ms')\n","\n","    def create_dataset(self, start_time, end_time, cols, prefix):\n","        c = copy.deepcopy(cols)\n","        if not prefix == '':\n","            for i in range(0, len(c)):\n","                c[i] = str(prefix) + str(c[i])\n","        timestamps = self.create_timestamps(start_time, end_time)\n","        self.data_table = pd.DataFrame(index=timestamps, columns=c)\n","\n","    # Add numerical data, we assume timestamps in the form of nanoseconds from the epoch\n","    def add_numerical_dataset(self, file, timestamp_col, value_cols, aggregation='avg', prefix=''):\n","        print(f'Reading data from {file}')\n","        #dataset = pd.read_csv(self.base_dir / file, skipinitialspace=True)\n","        dataset = pd.read_csv(file)\n","        #print(f'Reading data from')\n","        #dataset = pd.read_csv(io.BytesIO(uploaded[file]))\n","        #dataset = pd.read_csv(self.base_dir / file, skipinitialspace=True)\n","\n","        # Convert timestamps to dates\n","        dataset[timestamp_col] = pd.to_datetime(dataset[timestamp_col])\n","\n","        # Create a table based on the times found in the dataset\n","        if self.data_table is None:\n","            self.create_dataset(min(dataset[timestamp_col]), max(dataset[timestamp_col]), value_cols, prefix)\n","        else:\n","            for col in value_cols:\n","                self.data_table[str(prefix) + str(col)] = np.nan\n","\n","        # Over all rows in the new table\n","        for i in range(0, len(self.data_table.index)):\n","            # Select the relevant measurements.\n","            relevant_rows = dataset[\n","                (dataset[timestamp_col] >= self.data_table.index[i]) &\n","                (dataset[timestamp_col] < (self.data_table.index[i] +\n","                                           timedelta(milliseconds=self.granularity)))\n","            ]\n","            for col in value_cols:\n","                # Take the average value\n","                if len(relevant_rows) > 0:\n","                    if aggregation == 'avg':\n","                        self.data_table.loc[self.data_table.index[i], str(prefix)+str(col)] = np.average(relevant_rows[col])\n","                    else:\n","                        raise ValueError(f\"Unknown aggregation {aggregation}\")\n","                else:\n","                    self.data_table.loc[self.data_table.index[i], str(prefix)+str(col)] = np.nan\n","\n","    # Remove undesired value from the names.\n","    def clean_name(self, name):\n","        return re.sub('[^0-9a-zA-Z]+', '', name)\n","\n","    # Add data in which we have rows that indicate the occurrence of a certain event with a given start and end time.\n","    # 'aggregation' can be 'sum' or 'binary'.\n","    def add_event_dataset(self, file, start_timestamp_col, end_timestamp_col, value_col, aggregation='sum'):\n","        print(f'Reading data from {file}')\n","        dataset = pd.read_csv(self.base_dir / file)\n","\n","        # Convert timestamps to datetime.\n","        dataset[start_timestamp_col] = pd.to_datetime(dataset[start_timestamp_col])\n","        dataset[end_timestamp_col] = pd.to_datetime(dataset[end_timestamp_col])\n","\n","        # Clean the event values in the dataset\n","        dataset[value_col] = dataset[value_col].apply(self.clean_name)\n","        event_values = dataset[value_col].unique()\n","\n","        # Add columns for all possible values (or create a new dataset if empty), set the default to 0 occurrences\n","        if self.data_table is None:\n","            self.create_dataset(min(dataset[start_timestamp_col]), max(dataset[end_timestamp_col]), event_values, value_col)\n","        for col in event_values:\n","            self.data_table[(str(value_col) + str(col))] = 0\n","\n","        # Now we need to start counting by passing along the rows....\n","        for i in range(0, len(dataset.index)):\n","            # identify the time points of the row in our dataset and the value\n","            start = dataset[start_timestamp_col][i]\n","            end = dataset[end_timestamp_col][i]\n","            value = dataset[value_col][i]\n","            border = (start - timedelta(milliseconds=self.granularity))\n","\n","            # get the right rows from our data table\n","            relevant_rows = self.data_table[(start <= (self.data_table.index +timedelta(milliseconds=self.granularity))) & (end > self.data_table.index)]\n","\n","            # and add 1 to the rows if we take the sum\n","            if aggregation == 'sum':\n","                self.data_table.loc[relevant_rows.index, str(value_col) + str(value)] += 1\n","            # or set to 1 if we just want to know it happened\n","            elif aggregation == 'binary':\n","                self.data_table.loc[relevant_rows.index, str(value_col) + str(value)] = 1\n","            else:\n","                raise ValueError(\"Unknown aggregation '\" + aggregation + \"'\")\n","\n","    # This function returns the column names that have one of the strings expressed by 'ids' in the column name.\n","    def get_relevant_columns(self, ids):\n","        relevant_dataset_cols = []\n","        cols = list(self.data_table.columns)\n","\n","        for id in ids:\n","            relevant_dataset_cols.extend([col for col in cols if id in col])\n","\n","        return relevant_dataset_cols\n"]},{"cell_type":"markdown","id":"c5bd48fd","metadata":{"id":"c5bd48fd"},"source":["## Binary classification "]},{"cell_type":"markdown","id":"cce0059d","metadata":{"id":"cce0059d"},"source":["As you can see, the task is to correctly predict if a person is 'active' or not. For this purpose, the data was also preprocessed in the sense that all activities that were either 'running' or 'walking' were classified as active, whereas all other labels such as 'Sitting', 'WashingHands', 'Standing', 'Driving', 'Eating' were classified as inactive. A value of 1 for the feature was assigned if the user was active, and zero otherwise.\n"]},{"cell_type":"markdown","id":"bec97164","metadata":{"id":"bec97164"},"source":["## Importing the preprocessed data "]},{"cell_type":"markdown","id":"4d046ae1","metadata":{"id":"4d046ae1"},"source":["Make sure that you saved the data correctly and that you also refer to the right working directory."]},{"cell_type":"code","execution_count":null,"id":"cea563f2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72,"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","headers":[["content-type","application/javascript"]],"ok":true,"status":200,"status_text":""}}},"executionInfo":{"elapsed":56739,"status":"ok","timestamp":1635359325183,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"cea563f2","outputId":"79ff1643-c857-4b14-a48b-b66226c759eb"},"outputs":[],"source":["# If you work on a local machine, i.e. your laptop etc\n","#data_preprocessed250 = pd.read_csv('./intermediate_datafiles/data_250ns_no_time.csv', index_col=[0])\n","\n","from google.colab import files\n","uploaded = files.upload()\n","\n","#Now we need to make sure that you can convert it into a pandas dataframe!\n","\n","data_preprocessed250 = pd.read_csv(io.BytesIO(uploaded['data_250ns_no_time.csv']))\n","#Do not change this line of code. It removes an unnecessary column!\n","data_preprocessed250 = data_preprocessed250.drop([data_preprocessed250.columns[0]], axis = 1)\n"]},{"cell_type":"code","execution_count":null,"id":"c094025a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":219,"status":"ok","timestamp":1635359329991,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"c094025a","outputId":"640f13c9-36a0-46d2-b548-cc7674ff8b3d"},"outputs":[],"source":["data_preprocessed250.head()"]},{"cell_type":"markdown","id":"a8f0f7bb","metadata":{"id":"a8f0f7bb"},"source":["## Familiarize yourself with the data set "]},{"cell_type":"markdown","id":"8324ea02","metadata":{"id":"8324ea02"},"source":["The very first exercise before moving on to more advanced step in a machine learning pipeline is to familiarize yourself with the dataset and to get an overview of your data! A nice way is the .describe() method that gives you a very first overview of the data."]},{"cell_type":"code","execution_count":null,"id":"bbf3ead9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"elapsed":236,"status":"ok","timestamp":1635359334113,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"bbf3ead9","outputId":"aca35eff-96c4-444a-84e9-d07e12b358f5"},"outputs":[],"source":["data_preprocessed250.describe()"]},{"cell_type":"markdown","id":"10e972ef","metadata":{"id":"10e972ef"},"source":["This gives you a nice overview of some descriptive statistics of the dataset. The 25%, 50% and 75% represent the 25 percent, median and the 75% quartile. \n"]},{"cell_type":"markdown","id":"690edabd","metadata":{"id":"690edabd"},"source":["A nice way to visualize this is by means of a boxplot. Below you can find some code to run a boxplot. "]},{"cell_type":"code","execution_count":null,"id":"ee33a5a3","metadata":{"id":"ee33a5a3"},"outputs":[],"source":["red_circle = dict(markerfacecolor = 'red', marker = 'o')\n","list_of_features_displayed = ['acc_phone_x', 'acc_phone_y','acc_phone_z']#You can add some more here if you want\n","dataset = data_preprocessed250\n","\n","def plotting_boxplot(list_of_features_displayed, dataset):\n","    \n","    fig, axs = plt.subplots(1, len(list_of_features_displayed), figsize = (10,5))\n","    for i, ax in enumerate(axs.flat):\n","        list_of_features_displayed[i]\n","        ax.boxplot(dataset[list_of_features_displayed[i]], flierprops = red_circle)\n","        ax.set_title(list_of_features_displayed[i], fontsize = 10, fontweight = 'bold')\n","        ax.tick_params(axis = 'y', labelsize = 14)\n","    \n","    return plt.tight_layout()\n"]},{"cell_type":"code","execution_count":null,"id":"22ee1cfa","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"elapsed":1025,"status":"ok","timestamp":1635359341099,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"22ee1cfa","outputId":"320ccc11-21f4-4e9a-adf8-e92ab069334d"},"outputs":[],"source":["#Now you can plot and customize your plots!\n","\n","plotting_boxplot(list_of_features_displayed, dataset)"]},{"cell_type":"markdown","id":"5785bbba","metadata":{"id":"5785bbba"},"source":["## Handling outliers and missing data"]},{"cell_type":"markdown","id":"958e827c","metadata":{"id":"958e827c"},"source":["An important step when preprocessing and preparing your data for further analysis is how remove outliers and also to handle missing data. According to Grubbs (1969), outliers are defined as follows:\n","\n","**Outlier**: An outlier is an observation point that is distant from other observations.\n","\n","As you might think, the definition what is distant is crucial for detecting outliers. Hence, this is something that you need to decide on. Also, there are plenty of different approaches that one can could consider for detecting outliers. In prinicple, there are two different roads/streams when it comes to outlier detection:\n","\n","- distance based measures (use distance as a tool decide whether an obersation is an outlier or not)\n","- distribution based measures (assume an underlying distribution for the data generation process based on which an outlier is detected)\n","\n","\n","\n"]},{"cell_type":"markdown","id":"45b821bb","metadata":{"id":"45b821bb"},"source":["For both approaches, it is important to have scaled the dataset, i.e. it removes for each variable the mean and scales it to have unit variance. **Why do you think this is important?** \n","\n","The function below will scale all numerical data (therefore, the labeling data will not get scaled)"]},{"cell_type":"code","execution_count":null,"id":"a2819753","metadata":{"id":"a2819753"},"outputs":[],"source":["class standardizer:\n","    def __init__(self, data):\n","        self.data = data\n","        self.a = [col for col in self.data.columns if not col.startswith('Active')]\n","        self.b = [col for col in self.data.columns if col.startswith('Active')]\n","        self.column_names = self.a + self.b\n","    \n","    def fit(self):\n","        self.ct = ColumnTransformer([\n","        ('scaling_numerical', StandardScaler(), [col for col in self.data.columns if not col.startswith('Active')])\n","        ], remainder='passthrough')\n","        #return self.data\n","        self.ct.fit(self.data)\n","        return self.ct\n","        print('The column transformer is fit now')\n","    \n","    def get_col(self):\n","        return self.column_names"]},{"cell_type":"code","execution_count":null,"id":"2d616560","metadata":{"id":"2d616560"},"outputs":[],"source":["st = standardizer(data_preprocessed250)\n","my_standardizer= st.fit()\n","col_names = st.get_col()\n","\n","#New data frame\n","\n","data_scaled_250 = pd.DataFrame(my_standardizer.transform(data_preprocessed250), columns = col_names)"]},{"cell_type":"code","execution_count":null,"id":"29e4bf4f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1635359371057,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"29e4bf4f","outputId":"f3412f5d-ebcb-4897-bc76-218429483423"},"outputs":[],"source":["data_scaled_250.head()"]},{"cell_type":"markdown","id":"81edaea9","metadata":{"id":"81edaea9"},"source":["**Exercise**\n","\n","Inspect the data and see the changes compared to the previous data set. Use for this again the 'dataset_name'.describe() and 'dataset_name'.summary() method!"]},{"cell_type":"markdown","id":"69a51b36","metadata":{"id":"69a51b36"},"source":["Before moving on to remove outliers, it is also important to handle missing data. That is, for some observation there is no data available. This might result from technical issues, or even from some laziness from the person recording the data (for example, I forgot to label my data as 'On the Table'). In any way, we need find methods to impute missing values. Again, and as often times in machine learning, there are many options from which people can choose from. As always, it is important to consider the options and choose based upon your data and your knowledge!\n","\n","There are 'simple' options for numerical and categorical data to choose from:\n","Replace missing values with \n","- mean\n","- median\n","\n","**Question** What is a main advantage of the median compared to the mean for imputing the data? How could you check that (**Tip, you have already seen a way to see if a median might be more appropriate**).\n","\n","For categorical data people often use the mode (the most frequent value).\n","\n","Alternatively, one could use more advanced approaches, such as parametric approaches (fitting a model such as linear regression). Do not worry if you do not know all of these, it is only for you important to know that there are alternativ ways to impute missing data."]},{"cell_type":"markdown","id":"9cc5ef9e","metadata":{"id":"9cc5ef9e"},"source":["The following function visualizes the missing value for the dataset!"]},{"cell_type":"code","execution_count":null,"id":"130af3dd","metadata":{"id":"130af3dd"},"outputs":[],"source":["def visualize_missing_values(dataset):\n","    nulls = dataset.isnull().sum()\n","    sns.set(style=\"whitegrid\")\n","    ax = sns.barplot(x=nulls.index, y=nulls.values/len(dataset))\n","    #ax = ax.set(label='common xlabel', ylabel='common ylabel')\n","    ax= ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n","    return ax"]},{"cell_type":"code","execution_count":null,"id":"a38d357b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"executionInfo":{"elapsed":602,"status":"ok","timestamp":1635359376465,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"a38d357b","outputId":"3dad74e7-e07b-4ac4-bc47-0555e34846eb"},"outputs":[],"source":["visualize_missing_values(data_scaled_250)"]},{"cell_type":"markdown","id":"ab18425b","metadata":{"id":"ab18425b"},"source":["For preparing the dataset, write here the list for numerical and categorical data!"]},{"cell_type":"code","execution_count":null,"id":"1a211ddd","metadata":{"id":"1a211ddd"},"outputs":[],"source":["#Write here the list of the features!# Alternative you can also try to code it :)\n","numbers = ['acc_phone_x', ... ]\n","categ = ['...']"]},{"cell_type":"code","execution_count":null,"id":"c87c7828","metadata":{"id":"c87c7828"},"outputs":[],"source":["class Missing_Values:\n","    def __init__(self, dataset, method = 'mean'):\n","        self.method = method\n","        self.dataset = dataset\n","        self.numbers = [col for col in self.dataset.columns if not col.startswith('Active')]\n","        self.categ = [col for col in self.dataset.columns if col.startswith('Active')]\n","        self.labels = self.numbers + self.categ\n","        print(self.labels)\n","        \n","    def fit_imputer(self):\n","        #Build a pipeline for each type of data, one for numeric and one for categorical data\n","        categ_transformer = Pipeline(steps = [\n","            ('imputer', SimpleImputer(strategy = 'most_frequent'))\n","        ])\n","\n","        number_transformer = Pipeline(steps=[\n","            ('imputer', SimpleImputer(strategy = self.method)) \n","        ])\n","\n","        #Next step: Make the column transformer:\n","        preprocessor = ColumnTransformer(\n","            transformers = [\n","            ('num', number_transformer, self.numbers), \n","            ('categ',categ_transformer, self.categ)])\n","    \n","        #self.imputer = preprocessor.fit(self.dataset)\n","        a = preprocessor.fit_transform(self.dataset)\n","        b = preprocessor.fit(self.dataset)\n","        return b\n","        print('fit was performed')\n","    \n","    def get_col(self):\n","        return self.labels"]},{"cell_type":"code","execution_count":null,"id":"f05661cb","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1635359383296,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"f05661cb","outputId":"8797654f-ebcf-429a-d054-dbb45c3a1c55"},"outputs":[],"source":["miss_imputer = Missing_Values(data_scaled_250)\n","my_imputer = miss_imputer.fit_imputer()\n","col_names = miss_imputer.get_col()\n","\n","#New data frame\n","\n","data_scaled_250_new = pd.DataFrame(my_imputer.transform(data_scaled_250), columns = col_names)\n","data_scaled_250_new.head()\n","\n"]},{"cell_type":"markdown","id":"b802d002","metadata":{"id":"b802d002"},"source":["Once we have dealt with the scaling, we can detect the outliers! As described above, there are different ways of for detecting outliers. The following outlier class has two options from which you can choose from, the so-called local outlier factor as well as the the chauvenets criterion that represents a distribution based method. Most importantly, it assumes the data to be normally distributed! You can check the data (for here now only visually) and see if it approximately normally distributed."]},{"cell_type":"markdown","id":"a463dfd6","metadata":{"id":"a463dfd6"},"source":["The following class will give you some options to play around with:\n","- visualizing a feature and plot it\n","- do a Q-Q plot \n","- do a statistical test for testing if features are normally distributed. Here we will use the Shapiro-Wilk test.\n","\n","The Shapiro Wilk test returns a so-called p-value. It assumes first that the data is normally distributed. If the p-value falls below a certain threshold, the so-called significance level, people say that there is evidence against the null hypothesis and will reject than the hypothesis. Normal threshold levels, alpha levels, are 0.01, 0.05 and 0.10. Check the data and compare the p-values with the alpha value that you decide on! The lower the alpha level, the 'more strict' you are! If you use the option 'answer', it will show you the answer based on the alpha level that you chose! The default value is 0.05 as it is commonly used in Statistics.\n","\n","Play around with it and see if your data is normally distributed! "]},{"cell_type":"code","execution_count":null,"id":"d3f0c17d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1635359386494,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"d3f0c17d","outputId":"9c6634e6-7bf0-4842-cc98-1d60ea0ce358"},"outputs":[],"source":["data_scaled_250_new.head()"]},{"cell_type":"code","execution_count":null,"id":"94af74d0","metadata":{"id":"94af74d0"},"outputs":[],"source":["class visualizer_normality:\n","    def __init__(self, dataset, feature_list):\n","        self.features = feature_list\n","        self.length = len(feature_list)\n","        self.data = dataset\n","    def qq_plot(self):\n","        for i in range(0, self.length):\n","            axs[i] = qqplot(data = self.data[self.features[i]].values, line = 's')\n","            plt.title(f'QQ plot for {self.features[i]}')\n","            plt.tight_layout()\n","    def density_plot(self):\n","        fig, ax = plt.subplots(self.length, tight_layout = True)\n","        for i in range(0, self.length):\n","            sns.kdeplot(self.data[self.features[i]].values, bw_method=0.5, ax = ax[i])\n","            ax[i].title.set_text(f'Density plot for {self.features[i]}')\n","    def statistical_test(self, alpha = 0.05, answer = False):\n","        output_no_answer = dict()\n","        output_answer = dict()\n","        for i in range(0, self.length):\n","            p_value = shapiro(self.data[self.features[i]].values)[0]\n","            if p_value  < alpha:\n","                answer_test = 'The test suggests that the feature is not normally distributed'\n","            else:\n","                answer_test = 'The test suggests that the feature is normally distributed'\n","            output_answer[self.features[i]] = (p_value, answer_test)\n","            output_no_answer[self.features[i]] = (p_value)\n","        if answer == False:\n","            return output_no_answer\n","        else:\n","            return output_answer"]},{"cell_type":"markdown","id":"jJnVXs6MwLrl","metadata":{"id":"jJnVXs6MwLrl"},"source":["Inspect the features of the dataset and check if they are normally distributed! This is important to know for you as some outlier detection methods assume normal distribution!"]},{"cell_type":"code","execution_count":null,"id":"9d998dca","metadata":{"id":"9d998dca"},"outputs":[],"source":["#Input here what features you want to inspect! \n","feature_list = ['acc_phone_x', 'acc_phone_y', 'acc_phone_z']"]},{"cell_type":"code","execution_count":null,"id":"db2768f0","metadata":{"id":"db2768f0"},"outputs":[],"source":["my_visualizer = visualizer_normality(data_scaled_250_new, feature_list)"]},{"cell_type":"code","execution_count":null,"id":"ca9e4902","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":327},"executionInfo":{"elapsed":2136,"status":"ok","timestamp":1635359396518,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"ca9e4902","outputId":"69d46a54-92f4-479c-ea75-27c4b88fe36a"},"outputs":[],"source":["my_visualizer.density_plot()\n","p = my_visualizer.statistical_test(answer= True)"]},{"cell_type":"markdown","id":"33e24f60","metadata":{"id":"33e24f60"},"source":["Now that we are done with handling missing data and checking we can move on to remove outliers! To recall, there are two main categories of outlier detection:\n","\n","- distance based\n","- distribution based\n","\n","Here, we will provide you with the option to choose between a gaussian mixture model (assuming a Multinomial distribution) for the distribution based method, and the so-called local outlier factor as an example of a distance-based method. Try them out and see if the methods produce different results! \n","\n","Both models ask you to use a feature based on which it will detect an outlier. The Gaussian Mixture model will also ask you to input a threshold. This number should be an integer which represents the percentile. Normally, you should go for a low one such as 1,5, or maybe 10. The number of components is also a very crucial parameter, as it basically tells you how many normal distributions do we have! It will return a probability again similar to what we have seen before. \n","\n","In a next step, you can visualize the inlier and outlier points so you can check it visually if you did a good job!"]},{"cell_type":"markdown","id":"d9fbf84d","metadata":{"id":"d9fbf84d"},"source":["After you have decided on a method to remove any outliers (or maybe you decided to keep all data), you can use the remove method to remove any outlier data."]},{"cell_type":"code","execution_count":null,"id":"eeec4579","metadata":{"id":"eeec4579"},"outputs":[],"source":["class Outlier_detection:\n","    def __init__(self, dataset):\n","        self.data = dataset\n","    \n","    def chauvenet_criterion(self, feature, c):\n","        mean = self.data[feature].mean()\n","        std = self.data[feature].std()\n","        N = len(self.data[feature].values)\n","        threshold_chauv = 1/(c*N)\n","        assigned_value = []\n","        for i in range(0, N):\n","            value = scipy.stats.norm(mean, std).cdf(self.data[feature][i])\n","            if (value < threshold_chauv) or ((1-value) < threshold_chauv):\n","                assigned_value.append(-1)\n","            else:\n","                assigned_value.append(1)\n","        self.data['Chauvenet Outlier'] = assigned_value\n","        \n","    def local_outlier_fit(self, number_of_obs):\n","        self.lof = LocalOutlierFactor(n_neighbors = number_of_obs)\n","        print('The local outlier factor has been fit!')\n","        self.data['LOF Outlier'] = self.lof.fit_predict(self.data)\n","        \n","            \n","    def gaussian_mixture_fit(self, feature, components, threshold):\n","        self.gm = GaussianMixture(n_components = components)\n","        gm_data = self.data[feature].values.reshape(-1,1)\n","        self.gm.fit(gm_data)\n","        self.data['GM score'] = self.gm.score_samples(gm_data)\n","        threshold = np.percentile(self.data['GM score'].values, threshold)\n","        self.data['GM Outlier'] = np.where(self.data['GM score'] < threshold, -1, 1)\n","        print('The Gaussian mixture model has predicted the labels!')\n","        \n","\n","    def visualize_outlier(self, feature, method_used):\n","        if method_used == 'local_outlier':\n","            sns.scatterplot(x = self.data.index.values, y = self.data[feature], data = self.data, hue = 'LOF Outlier')\n","        elif method_used == \"gaussian_mixture\":\n","            sns.scatterplot(x = self.data.index.values, y = self.data[feature], data = self.data, hue = 'GM Outlier')\n","        elif method_used == 'chauvenet_criterion':\n","            sns.scatterplot(x = self.data.index.values, y = self.data[feature], data = self.data, hue = 'Chauvenet Outlier')\n","            \n","    def return_data(self):\n","        return self.data\n","    \n","    def return_data_outlier_removed(self, method):\n","        if method == 'local_outlier':\n","            return_data = self.data[self.data['LOF Outlier'] != -1].reset_index(drop = True)\n","            return return_data.drop(['LOF Outlier','GM Outlier','GM score','Chauvenet Outlier'], axis = 1)\n","        elif method == 'gaussian_mixture':\n","            return_data = self.data[self.data['GM Outlier'] != -1].reset_index(drop = True)\n","            return return_data.drop(['LOF Outlier','GM Outlier','GM score','Chauvenet Outlier'], axis = 1)\n","        elif method == 'chauvenet_criterion':\n","            return_data = self.data[self.data['Chauvenet Outlier'] != -1].reset_index(drop = True)\n","            return return_data.drop(['LOF Outlier','GM Outlier','GM score','Chauvenet Outlier'], axis = 1)\n","    \n","    def return_data_outlier_replaced(self, outlier_column, imputer):\n","        self.label = [col for col in self.data.columns if col.startswith('Active')]\n","        self.xdata = self.data.loc[:, ~self.data.columns.isin(self.label)]\n","        self.ydata = self.data.loc[:, self.label]\n","        xmean = self.xdata.groupby(outlier_column, as_index = False).agg('mean')\n","        xmean_values = xmean.loc[1, xmean.columns != outlier_column].values\n","        index_replace = list(self.xdata[self.xdata[outlier_column] == -1].index)\n","        self.xdata.loc[index_replace, ~self.xdata.columns.isin([outlier_column])] = xmean_values\n","        \n","        self.new_data = pd.concat([self.xdata, self.ydata], axis = 1)\n","        return self.new_data.drop(['LOF Outlier','GM Outlier','GM score','Chauvenet Outlier'], axis = 1)\n","            \n","    def return_outlier_mean_description(self, outlier_column):\n","        self.label = [col for col in self.data.columns if col.startswith('Active')]\n","        self.xdata = self.data.loc[:, ~self.data.columns.isin(self.label)]\n","        return self.xdata.groupby(outlier_column, as_index = False).mean()\n","            "]},{"cell_type":"code","execution_count":null,"id":"766e3888","metadata":{"id":"766e3888"},"outputs":[],"source":["dataset_new = data_scaled_250_new.copy()"]},{"cell_type":"code","execution_count":null,"id":"90886e69","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31739,"status":"ok","timestamp":1635359441209,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"90886e69","outputId":"a763db92-3f17-4233-dfd7-4efea647fae6"},"outputs":[],"source":["outlier_det = Outlier_detection(dataset_new)\n","outlier_det.local_outlier_fit(20)\n","outlier_det.gaussian_mixture_fit('acc_phone_x',3,5)\n","outlier_det.chauvenet_criterion('acc_phone_x', 2)"]},{"cell_type":"markdown","id":"H5M7JA7BxwuU","metadata":{"id":"H5M7JA7BxwuU"},"source":["Play around here! Visualize after you have fit the algorithms the outliers. Do you agree with the algorithm? Maybe you do not want to remove anything at all? That is also fine! It is always important to visualize and convinve yourself with the results of an algorithm. The following code visualizes the results of the LOF outlier and plots it for the 'acc_phone_x' value!"]},{"cell_type":"code","execution_count":null,"id":"gU9fEVJ0ySXt","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":268},"executionInfo":{"elapsed":3686,"status":"ok","timestamp":1635359447647,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"gU9fEVJ0ySXt","outputId":"31a527eb-c7b7-42d1-c157-65de2ba59bdd"},"outputs":[],"source":["outlier_det.visualize_outlier('acc_phone_x', 'local_outlier')"]},{"cell_type":"markdown","id":"HN1AKcppym1Z","metadata":{"id":"HN1AKcppym1Z"},"source":["After we have visualized some of the results, we need to proceed and remove outliers. You can decide on one method or as I said before decide to not do anything! Also, when you remove outliers, you can either decide to remove them completely or replace it with a mean or median value. Again, think of the pros/cons of each of the two methods!\n"]},{"cell_type":"code","execution_count":null,"id":"7cbd9cac","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1635359450847,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"7cbd9cac","outputId":"b7b1a7cc-a250-46a6-f5bb-de1c5dcb742d"},"outputs":[],"source":["dataset_new = outlier_det.return_data_outlier_replaced('LOF Outlier', 'mean')"]},{"cell_type":"code","execution_count":null,"id":"d22bbae7","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1635359452985,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"d22bbae7","outputId":"5829617d-fa09-4ff1-bf78-c9e615c00b7b"},"outputs":[],"source":["dataset_new.head()"]},{"cell_type":"markdown","id":"ff5a7b6e","metadata":{"id":"ff5a7b6e"},"source":["## Handling noise - PCA decomposition "]},{"cell_type":"markdown","id":"4049e09b","metadata":{"id":"4049e09b"},"source":["For the principal component, the algorithm tries to reduce the number of features (here we have about 30 now even) by 'mapping'/aligning the features into a direction in which the variables vary the most. It is quite tough to imagine this, but the following video helps and explains the algorithm in more detail. **You do not need to watch it, but if you are curious and wanna get a better understanding, it might help!**\n","\n","[The link to the Youtube video](https://www.youtube.com/watch?v=FgakZw6K1QQ&t=1126s)\n","\n","Of course you should start from the start with the video. Watch the video though only if you have time! After all it is a Hackathon!"]},{"cell_type":"markdown","id":"ad79c545","metadata":{"id":"ad79c545"},"source":["The following code implements the pca algorithm. One important consideration for you to consider is how many so-called principal components do you want to keep. For this reason, the plot 'explained variance' will help you figuring this out. **Decide in your group if you want to use the algorithm at all and if so, how many features you want to use!**.\n","\n","An important aspect you need to be aware of is that you cannot intrepret the principal components any more. This is a big drawback of the method!"]},{"cell_type":"code","execution_count":null,"id":"b2870fd4","metadata":{"id":"b2870fd4"},"outputs":[],"source":["#Important go through the methods step by step!\n","\n","class pca_reduction:\n","    def __init__(self, data, features = None):\n","        self.features = features\n","        self.data = data\n","        self.numerical = [col for col in self.data.columns if not col.startswith('Active')] #Remove final label to be predicted\n","        self.ylabel = [col for col in self.data.columns if col.startswith('Active')]\n","        self.xdata_orig = self.data.loc[:, self.numerical]\n","        self.xdata = self.data.loc[:, self.numerical]\n","        self.ydata = self.data.loc[:, self.ylabel]\n","        self.pca = PCA()\n","\n","    def fit_pca(self, data_to_fit = None):\n","        \n","        if self.features is None:\n","            self.pca.fit(self.xdata_orig)\n","        else:\n","            self.pca.fit(self.xdata_orig[features])\n","        self.components_pca = self.pca.components_.shape[1]\n","        self.explained_variance = self.pca.explained_variance_ratio_\n","        print('PCA was fit successfully to the features!')\n","        \n","        if data_to_fit is None:\n","            self.transformed_values = self.pca.transform(self.xdata_orig)\n","       \n","        else:\n","            self.xdata_fit = data_to_fit.loc[:, self.numerical]\n","            self.transformed_values = self.pca.transform(self.xdata_fit)\n","            \n","    def visualize_pca_components(self):\n","        \n","        self.components_plot = np.arange(1, self.components_pca+1,1)\n","        sns.set(style=\"whitegrid\")\n","        ax = sns.barplot(x=self.components_plot, y=self.explained_variance)\n","        ax.set_title('Results PCA analysis')\n","        ax= ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n","                    \n","    def return_pca_components(self, number_comp, data_to_fit = None, fit = False, attach_to_dataset = True):\n","        if fit == True and data_to_fit is not None:\n","            self.xdata = data_to_fit.loc[:, self.numerical]\n","            self.ydata = data_to_fit[self.ylabel]\n","            for comp in range(0, number_comp):\n","                self.xdata['pca_' +str(comp+1)] = self.transformed_values[:,comp]\n","\n","        #elif attach_to_dataset == True:\n","        else:\n","            #And add the new ones:\n","            for comp in range(0, number_comp):\n","                self.xdata['pca_' +str(comp+1)] = self.transformed_values[:,comp]\n","            \n","        return pd.concat([self.xdata, self.ydata], axis = 1).dropna()"]},{"cell_type":"code","execution_count":null,"id":"f52c7459","metadata":{"id":"f52c7459"},"outputs":[],"source":["dataset_NEW = dataset_new.copy()"]},{"cell_type":"code","execution_count":null,"id":"38c4728a","metadata":{"id":"38c4728a"},"outputs":[],"source":["abc = dataset_new.copy()"]},{"cell_type":"code","execution_count":null,"id":"fc3da9b5","metadata":{"id":"fc3da9b5"},"outputs":[],"source":["data_pca = dataset_new.copy()"]},{"cell_type":"code","execution_count":null,"id":"c15e8273","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"executionInfo":{"elapsed":858,"status":"ok","timestamp":1635359510000,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"c15e8273","outputId":"3a48a277-ef21-4b22-8a03-6acaf21ed855"},"outputs":[],"source":["my_pca = pca_reduction(dataset_NEW)\n","my_pca.fit_pca()\n","my_pca.visualize_pca_components()"]},{"cell_type":"code","execution_count":null,"id":"e913aa15","metadata":{"id":"e913aa15"},"outputs":[],"source":["data_tada = my_pca.return_pca_components(1)"]},{"cell_type":"code","execution_count":null,"id":"4d615866","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1635359514262,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"4d615866","outputId":"ae9177e9-1a4b-432b-8fba-d31638d1a2da"},"outputs":[],"source":["data_tada.head()"]},{"cell_type":"markdown","id":"0f6417c1","metadata":{"id":"0f6417c1"},"source":["# Part 2: Feature engineering "]},{"cell_type":"markdown","id":"09644e02","metadata":{"id":"09644e02"},"source":["In this section, we will consider a few options for you to add new features. This is in practice an important field, but one also needs to be creative and/or use domain knowledge (that is, one has a certain expertise in the field) when defining new variables. Here we will consider a very basic option for you. To recall, each observation in the dataset represents an observation in time. One possible way to generate a new feature is to take the mean (or median, max, min) over a specific time window. This could be for example 5. This means I would summarize the information of 5 time observations in one new feature! \n","The functions that you can insert are 'mean', 'max', 'min', 'std'.\n","\n","Check the following code out and see what features you want to add and also what aggregate function and time window (this is quite important. Maybe think also about the granularity of the dataset!).\n","\n","One way to see if you should go for a specific time window is to plot values and see how 'smooth' the line is for a certain window size. Try it!\n","\n","For instance, go for window size = 20 (which means 5 seconds, as 4 samples represent a datapoint). Alternatively, you can go for 120, which is 30 seconds! However try out several things!"]},{"cell_type":"code","execution_count":null,"id":"91ac93f2","metadata":{"id":"91ac93f2"},"outputs":[],"source":["class rolling_window_aggregator:\n","    def aggregate(self,data, window, aggregation_function):\n","        self.data = data\n","        #Keep only numerical data\n","        numerical = [col for col in self.data.columns if not col.startswith('Active')]\n","        new_col = [col + aggregation_function for col in numerical]\n","        data_num = self.data.loc[:, numerical] \n","        if aggregation_function == 'mean':\n","            data_num.rolling(window).mean()\n","            data_num.columns = new_col\n","            return data_num\n","        \n","        elif aggregation_function == 'max':\n","            data_num.rolling(window).max()\n","            data_num.columns = new_col\n","            return data_num\n","        \n","        elif aggregation_function == 'min':\n","            data_num.rolling(window).min()\n","            data_num.columns = new_col\n","            return data_num\n","            \n","        elif aggregation_function == 'median':\n","            data_num.rolling(window).median()\n","            data_num.columns = new_col\n","            return data_num\n","        \n","        elif aggregation_function == 'std':\n","            data_num.rolling(window).std()\n","            data_num.columns = new_col\n","            return data_num\n","        \n","    def concat_data(self, dataset_original, dataset_2):\n","        self.data_new = pd.concat([dataset_original, dataset_2], axis = 1)\n","        return self.data_new\n","    \n","    def visualize_plot(self, feature_to_plot):\n","        plt.plot(np.arange(0, len(self.data_new[feature_to_plot])), self.data_new[feature_to_plot])"]},{"cell_type":"markdown","id":"cb6cf410","metadata":{"id":"cb6cf410"},"source":["To include more than one aggregation feature in your original dataset, use the method 'aggregate' as many times as you want to create new features. For instance, use the 'aggregate method' twice for mean and std. deviation. Then, you can start to put your data together. This is done by using 'concat_data'. \n","\n","**Important** \n","First concate data data together that only has the new features, i.e. those were the outputs of the method 'agggregate'. In a last step, concate your original data to the just concated dataframe.\n","\n","Below you find an example of how things could work out!"]},{"cell_type":"code","execution_count":null,"id":"a3dcfeee","metadata":{"id":"a3dcfeee"},"outputs":[],"source":["testy = data_tada.copy()"]},{"cell_type":"code","execution_count":null,"id":"1912cace","metadata":{"id":"1912cace"},"outputs":[],"source":["class_roll = rolling_window_aggregator()\n","testy_mean = class_roll.aggregate(testy, 120, 'mean')\n","testy_std = class_roll.aggregate(testy, 120, 'std')"]},{"cell_type":"code","execution_count":null,"id":"b8ce092f","metadata":{"id":"b8ce092f"},"outputs":[],"source":["new1 = class_roll.concat_data(testy_std, testy_mean)"]},{"cell_type":"code","execution_count":null,"id":"2edda215","metadata":{"id":"2edda215"},"outputs":[],"source":["NEW_dataset = class_roll.concat_data(testy, new1)"]},{"cell_type":"code","execution_count":null,"id":"18dee294","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":296,"status":"ok","timestamp":1635356931231,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"18dee294","outputId":"1b59a398-0634-4c8e-e2c9-a19b6b3213d1"},"outputs":[],"source":["NEW_dataset.columns"]},{"cell_type":"markdown","id":"027e2299","metadata":{"id":"027e2299"},"source":["## Dimensionality reduction / Feature selection (OPTIONAL)"]},{"cell_type":"markdown","id":"4bae8939","metadata":{"id":"4bae8939"},"source":["**Important note**:\n","- Feature selection as it is illustrated below should be normally part of the cross-validation used below (when we train the models and use GridSearchCV). Hence, using feature selection before using cross-validation is not suggested and therefore the code below is more for you to play around and for educational purposes!"]},{"cell_type":"markdown","id":"7fc67575","metadata":{"id":"7fc67575"},"source":["Given that we have now a lot of features, a lot of people use techniques to reduce the numbers of features in a smart way. We will have a look at two methods, i.e.\n","\n","- Principal components\n","- Feature selection via Forward selection\n","\n","We have seen the principal component analysis algorithm already before, so here we will focus on the Feature selection method. Here, the idea is that we will figure out step-by-step which features are more useful from a given set of features. We will first check among all features which is the one that will help us the most with the classification task at hand. After we have found one, we will continue our search with adding a second one (to the one that we have already found). \n","\n","In the following list, you can add the variables that you want to select from. This will be the input of the algorithm. "]},{"cell_type":"code","execution_count":null,"id":"53c15109","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1635359533449,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"53c15109","outputId":"3824bf48-9566-4d07-8416-9f75538ad07f"},"outputs":[],"source":["#Writting down the variables for the feature selection process\n","#Write here the features that you want to input in the algorithm\n","feature_list = ['acc_phone_x', 'acc_phone_y', 'acc_phone_z']\n","\n","#Alternatively, you could just use the following command to get all features if you want to test all\n","feature_list = [col for col in NEW_dataset.columns if col != 'Active']\n","len(feature_list)"]},{"cell_type":"code","execution_count":null,"id":"3b61093e","metadata":{"id":"3b61093e"},"outputs":[],"source":["class feature_selection_algorithm:\n","    def __init__(self, data, feature_list):\n","        self.data = data\n","        self.feature_list = feature_list\n","        self.ydata = data['Active'].values\n","        self.xdata = self.data.loc[:,feature_list]\n","    def fit_selection(self, number_of_features_to_select):\n","        self.selector = SelectKBest(f_classif,k=number_of_features_to_select).fit(self.xdata,self.ydata)\n","    def transform(self):\n","        self.support = self.selector.get_support()\n","        self.columns = [key for key, value in dict(zip(self.feature_list, self.support)).items() if value == True ]\n","        self.dfydata = pd.DataFrame(self.ydata, columns = ['Active'])\n","        self.dfxdata = pd.DataFrame(self.selector.transform(self.xdata), columns = self.columns)\n","        return pd.concat([self.dfxdata, self.dfydata], axis = 1)"]},{"cell_type":"code","execution_count":null,"id":"fcb8b7e4","metadata":{"id":"fcb8b7e4"},"outputs":[],"source":["ft_selector = feature_selection_algorithm(NEW_dataset, feature_list)\n","ft_selector.fit_selection(10)"]},{"cell_type":"code","execution_count":null,"id":"f18d95af","metadata":{"id":"f18d95af"},"outputs":[],"source":["final_data = ft_selector.transform()"]},{"cell_type":"code","execution_count":null,"id":"9e5648a7","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1635359566318,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"9e5648a7","outputId":"6216af9e-3de7-4971-85e5-6365914f4255"},"outputs":[],"source":["final_data"]},{"cell_type":"markdown","id":"b686f9ea","metadata":{"id":"b686f9ea"},"source":["# Part 3: Training your model"]},{"cell_type":"markdown","id":"9c4502c9","metadata":{"id":"9c4502c9"},"source":["After the final steps, we can finally start to begin to train our model and check our predictions! Yet, there is an important first step that we need to do before starting to train the model. "]},{"cell_type":"markdown","id":"df25670d","metadata":{"id":"df25670d"},"source":["## Handling unbalanced dataset "]},{"cell_type":"code","execution_count":null,"id":"bd17b6f3","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1635359570908,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"bd17b6f3","outputId":"492278e7-0f5a-40ab-9b92-6d406a197128"},"outputs":[],"source":["NEW_dataset.head()"]},{"cell_type":"code","execution_count":null,"id":"fc3317dd","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":302},"executionInfo":{"elapsed":714,"status":"ok","timestamp":1635359574381,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"fc3317dd","outputId":"1ec4009e-2652-40f0-e6cd-447b287419a7"},"outputs":[],"source":["#Check the proportion of the classes that we want to estimate.\n","sns.histplot(NEW_dataset['Active'])"]},{"cell_type":"markdown","id":"2720feed","metadata":{"id":"2720feed"},"source":["As we can see, there is a huge so-called imbalanced in the dataset. There are way more instances of User being inactive than active! If an algorithm would now always predict 0, i.e. inactive, it would be correct in about 80% of the cases already. To prevent this from happening, there are several solutions:\n","\n","**Sampling approaches**\n","- upsampling the minority class\n","- downsampling the majority class\n","- create an artificial data set using the so-called SMOTE algorithm\n","\n","It is important to note that all sampling approaches are should be only applied to the training data set! So that is, we had split our data set into a train and test set, we would have to apply sampling approaches only to the training data set and **NOT** the test set!\n","\n","**Question for you**:\n","What are main disadvantages of the upsampling and the downsampling approach? \n","\n","**Performance metric approach**\n","- We can use a different evaluation metric. Instead of saying, how many times are we correct, we can also use a more balanced approach which also considers how often we were incorrect. For example the F1 score is a more balanced approach. Alternatively, there is the ROC-AUC measure. This is the one that we will use. The closer the score to 1, the better! A score of 0.5 would mean that we are at random guess, so the model is as good as flipping a coin!\n","\n","**Model approach**\n","- There are specific machine learning models that are more suited when dealing with imbalanced data. Those methods will focus on mistakes and will iteratively try to get them correct over the long run. The class of models that are in this category are called boosting algorithms.\n"]},{"cell_type":"markdown","id":"309349c9","metadata":{"id":"309349c9"},"source":["## Play around "]},{"cell_type":"markdown","id":"a10f10dd","metadata":{"id":"a10f10dd"},"source":["In the following code, you can choose one of the sampling approaches. Just play around and see what you want to use and combine it with some models."]},{"cell_type":"code","execution_count":null,"id":"11efdd1d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":541},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1635359578620,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"11efdd1d","outputId":"3356ca8e-a25f-44d9-905f-52d74e89a106"},"outputs":[],"source":["NEW_dataset"]},{"cell_type":"code","execution_count":null,"id":"f7c29924","metadata":{"id":"f7c29924"},"outputs":[],"source":["class imbalance_sampler:\n","    def __init__(self, data):\n","        self.data = data\n","        self.ydata = data['Active'].values\n","        self.xdata = self.data.drop(['Active'], axis=1)\n","    \n","    def smote(self):\n","        self.sm = SMOTE(random_state=12)\n","        self.x_train_sm, self.y_train_sm = self.sm.fit_resample(self.xdata, self.ydata)\n","        return self.x_train_sm, self.y_train_sm\n","    \n","    def downsampling(self):\n","        self.downsampler = RandomUnderSampler(random_state=0)\n","        self.x_train_ds, self.y_train_ds = self.downsampler.fit_resample(self.xdata, self.ydata)\n","        return self.x_train_ds, self.y_train_ds\n","    \n","    def oversampling(self):\n","        self.oversampler = RandomOverSampler(random_state=0)\n","        self.x_train_os, self.y_train_os = self.oversampler.fit_resample(self.xdata, self.ydata)\n","        return self.x_train_os, self.y_train_os        "]},{"cell_type":"code","execution_count":null,"id":"01988c18","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":541},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1635359629961,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"01988c18","outputId":"c8967a3e-e96f-470c-8e9a-e9a96c92a275"},"outputs":[],"source":["NEW_dataset"]},{"cell_type":"code","execution_count":null,"id":"950211b4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":810,"status":"ok","timestamp":1635359633044,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"950211b4","outputId":"39687b1b-2a74-40be-f53e-deb9ef294688"},"outputs":[],"source":["im = imbalance_sampler(NEW_dataset)\n","sm_data = im.smote()\n","ds_data = im.downsampling()\n","os_data = im.oversampling()"]},{"cell_type":"markdown","id":"18d51532","metadata":{"id":"18d51532"},"source":["The following class will implement different 'classical' models for you that are not particularly suited for imbalanced datasets. Yet, if you have done the previous step, you already have some adjusted data set!"]},{"cell_type":"markdown","id":"e6407c2f","metadata":{"id":"e6407c2f"},"source":["In the following, we will consider methods that are more suitable for imbalanced data sets. Hence we will consider three models (do no worry about the ins and outs of the models). You can google and read upon them if you want to get a further understanding. This workshop is just here for you to learn and see what is out there! You can train them by including your dataset that you have obtained before! In particular, you have two options:\n","\n","- Support Vector Machine (SVM)\n","- Logistic Regression (LR)\n","\n","The SVM model performs often times more superior compared to the LR, yet the latter has is better when it comes to explaining results! Yet, it takes way more computational time. Please be aware of this before trying it out!\n","\n","You can also try to use :\n","\n","- AdaBoost Classifier (ADA)\n","\n","which is a boosting algorithm!"]},{"cell_type":"code","execution_count":null,"id":"878f00aa","metadata":{"id":"878f00aa"},"outputs":[],"source":["class ml_model():\n","    def __init__(self, data, sampling_approach_before = True):\n","        if sampling_approach_before == True:\n","            self.xdata = data[0]\n","            self.ydata = pd.DataFrame(data[1], columns = ['Activity'])\n","            self.ydata = data[1]\n","        else:\n","            self.ydata = data['Active'] \n","            self.xdata = data.drop(['Active'], axis = 1) \n","    def fit_LR(self):\n","        #return self.xdata\n","        self.myLR = LogisticRegression(random_state = 0)\n","        self.my_grid_values_lr = {'penalty': ['l2','none'], 'solver': ['sag'], 'max_iter': [100]}\n","        print('Please wait the LR best model is going to be fit!')\n","        self.grid_myLR = GridSearchCV(self.myLR, param_grid = self.my_grid_values_lr, cv = 3, n_jobs = 5, scoring = 'roc_auc', error_score = 'raise')\n","        self.grid_myLR.fit(self.xdata, self.ydata)\n","        print('Best penalty parameter : '+ str(self.grid_myLR.best_estimator_.penalty))\n","        print('Best ROC AUC score : '+ str(self.grid_myLR.best_score_))\n","        \n","    def fit_SVM(self):\n","        self.mySVM = SVC(random_state = 0)\n","        self.my_grid_values_svm = {'kernel': ['linear'], 'C': [0, 1]}\n","        print('Please wait the best SVM model is going to be fit!')\n","        self.grid_mySVM = GridSearchCV(self.mySVM, param_grid = self.my_grid_values_svm, cv = 3, n_jobs = 5, scoring = 'roc_auc')\n","        self.grid_mySVM.fit(self.xdata, self.ydata)\n","        print('Best C parameter : '+ str(self.grid_mySVM.best_estimator_.C))\n","        print('Best ROC AUC score : '+ str(self.grid_mySVM.best_score_))\n","    \n","    def fit_ADA(self):\n","        self.myADA = AdaBoostClassifier(random_state = 0)\n","        self.my_grid_values_ada = {'n_estimators': [5,10,50,100]}\n","        print('Please wait the best ADA model is going to be fit!')\n","        self.grid_myADA = GridSearchCV(self.myADA, param_grid = self.my_grid_values_ada, cv = 3, n_jobs = 5, scoring = 'roc_auc')\n","        self.grid_myADA.fit(self.xdata, self.ydata)\n","        print('Best number of estimators parameter : '+ str(self.grid_myADA.best_estimator_.n_estimators))\n","        print('Best ROC AUC score : '+ str(self.grid_myADA.best_score_))\n","    \n","    def prediction_LR(self, xtest):\n","        return self.grid_myLR.predict(xtest)\n","    \n","    def prediction_SVM(self, xtest):\n","        return self.grid_mySVM.predict(xtest)\n","   \n","    def prediction_ADA(self, xtest):\n","        return self.grid_myADA.predict(xtest)\n","                                                                            \n","                                                                "]},{"cell_type":"code","execution_count":null,"id":"e4e2706f","metadata":{"id":"e4e2706f"},"outputs":[],"source":["vanilla_ada = ml_model(ds_data, sampling_approach_before = True)"]},{"cell_type":"code","execution_count":null,"id":"ab2ae5c6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18476,"status":"ok","timestamp":1635359660847,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"ab2ae5c6","outputId":"1c73e7ed-a164-4000-b9e9-2e22dafdcc84"},"outputs":[],"source":["vanilla_ada.fit_ADA()"]},{"cell_type":"markdown","id":"419a74b9","metadata":{"id":"419a74b9"},"source":["Also try to fit a logistic regression model following this exanple. Also you should try to fit a SVM model. Which one performs best?"]},{"cell_type":"markdown","id":"a373a96a","metadata":{"id":"a373a96a"},"source":["# Your turn! Train your own algorithm with your own data!"]},{"cell_type":"markdown","id":"ad207192","metadata":{"id":"ad207192"},"source":["Now it is time for you to go through all steps from before and use your own test data that you have collected!"]},{"cell_type":"markdown","id":"bdce2100","metadata":{"id":"bdce2100"},"source":["## Importing the training data set"]},{"cell_type":"markdown","id":"fca3dc73","metadata":{"id":"fca3dc73"},"source":["In the first step, you need to import the data that you collected via the software. For this, record the data via the 'Sensor Logger' app and export it. Tip: Name the data set right away as being active or not. To recall: active means either running or walking, whereas 'Sitting', 'WashingHands', 'Standing', 'Driving', 'Eating' were classified as inactive. A value of 1 for the feature was assigned if the user was active, and zero otherwise.\n","\n","You should export the dataset as ziped CSV! Then save it in a place of your choice and unzip the data file. As a result, you will get a folder (named the way you named your dataset). In that folder, you will find then four datasets ('Accelerometer', 'Gyroscope', 'Light' and 'Magnetometer'). Also, there is a metadata file. \n","\n","Do one activity for certain amount of seconds. Then save that individual data file and save this. Do each data set step by step, i.e. do not do several activities at the same time. Do for instance 10 seconds of walking. Save and export the data file. Convert that data file with the function below."]},{"cell_type":"markdown","id":"9e69a38c","metadata":{"id":"9e69a38c"},"source":["## Converting the raw data into aggregate data "]},{"cell_type":"markdown","id":"2cd2c51d","metadata":{"id":"2cd2c51d"},"source":["To convert the data into the format we need, we need to use the following function."]},{"cell_type":"markdown","id":"697abeab","metadata":{"id":"697abeab"},"source":["**Important**: Change the paths dataset_path, result,path, result_fname according to how you have saved the data!"]},{"cell_type":"code","execution_count":null,"id":"0zU5AEFP84xT","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174,"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","headers":[["content-type","application/javascript"]],"ok":true,"status":200,"status_text":""}}},"executionInfo":{"elapsed":35191,"status":"ok","timestamp":1635359704422,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"0zU5AEFP84xT","outputId":"4ac9494c-8765-476d-9da3-184393791af1"},"outputs":[],"source":["DATASET_PATH = files.upload()"]},{"cell_type":"code","execution_count":null,"id":"91b21898","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1694,"status":"ok","timestamp":1635359709746,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"91b21898","outputId":"68d1a98f-fcfb-411d-d066-eeca76ab3a24"},"outputs":[],"source":["## THIS IS THE CODE TO RUN TO CREATE AND CONVERT YOUR OWN DATASET!\n","\n","#DATASET_PATH = Path('./my_own_data/walking')\n","RESULT_PATH = Path('./intermediate_datafiles/')\n","\n","# CHANGE THE RESULT DATA FILE NAME!\n","RESULT_FNAME = 'data_own.csv'\n","\n","# Set a granularity (the discrete step size of our time series data). We'll use a course-grained granularity of one\n","# instance per minute, and a fine-grained one with four instances per second.\n","GRANULARITIES = [250]\n","\n","# Do not change this\n","use_c = ['acc_phone_x','acc_phone_y','acc_phone_z','gyr_phone_x','gyr_phone_y','gyr_phone_z','light_phone_lux','mag_phone_x','mag_phone_y','mag_phone_z','Active']\n","# Add your label based on if you were active = 1 or not 0\n","\n","label = 1\n","\n","# We can call Path.mkdir(exist_ok=True) to make any required directories if they don't already exist.\n","#[path.mkdir(exist_ok=True, parents=True) for path in [DATASET_PATH, RESULT_PATH]]\n","\n","\n","datasets = []\n","for milliseconds_per_instance in GRANULARITIES:\n","    print(f'Creating numerical datasets from files in {DATASET_PATH} using granularity {milliseconds_per_instance}.')\n","\n","    # Create an initial dataset object with the base directory for our data and a granularity\n","    dataset = CreateDataset(DATASET_PATH, milliseconds_per_instance)\n","\n","    # Add the selected measurements to it.\n","\n","    # We add the accelerometer data (continuous numerical measurements) of the phone and the smartwatch\n","    # and aggregate the values per timestep by averaging the values\n","    dataset.add_numerical_dataset('Accelerometer.csv', 'time', ['x','y','z'], 'avg', 'acc_phone_')\n","\n","    # We add the gyroscope data (continuous numerical measurements) of the phone and the smartwatch\n","    # and aggregate the values per timestep by averaging the values\n","    dataset.add_numerical_dataset('Gyroscope.csv', 'time', ['x','y','z'], 'avg', 'gyr_phone_')\n","\n","\n","    # We add the labels provided by the users. These are categorical events that might overlap. We add them\n","    # as binary attributes (i.e. add a one to the attribute representing the specific value for the label if it\n","    # occurs within an interval).\n","    #dataset.add_event_dataset('labels.csv', 'label_start', 'label_end', 'label', 'binary')\n","\n","    # We add the amount of light sensed by the phone (continuous numerical measurements) and aggregate by averaging\n","    dataset.add_numerical_dataset('Light.csv', 'time', ['lux'], 'avg', 'light_phone_')\n","\n","    # We add the magnetometer data (continuous numerical measurements) of the phone and the smartwatch\n","    # and aggregate the values per timestep by averaging the values\n","    dataset.add_numerical_dataset('Magnetometer.csv', 'time', ['x','y','z'], 'avg', 'mag_phone_')\n","\n","\n","    # Get the resulting pandas data table\n","    dataset = dataset.data_table\n","\n","    datasets.append(copy.deepcopy(dataset))\n","\n","    # If needed, we could save the various versions of the dataset we create in the loop with logical filenames:\n","    #dataset.to_csv(RESULT_PATH / f'data_preprocessed_own_data{milliseconds_per_instance}.csv')\n","\n","# Make a table like the one shown in the book, comparing the two datasets produced.\n","#util.print_latex_table_statistics_two_datasets(datasets[0], datasets[1])\n","\n","# Finally, store the last dataset we generated (250 ms).\n","# Add the label we have\n","dataset['Active'] = label\n","#dataset.to_csv(RESULT_PATH / RESULT_FNAME)\n","dataset_own = dataset.reset_index(drop = True)\n"]},{"cell_type":"markdown","id":"ed850136","metadata":{"id":"ed850136"},"source":["Now you can have a look at your data! Import your dataset via the following command"]},{"cell_type":"code","execution_count":null,"id":"simSvNCI_l6C","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":988,"status":"ok","timestamp":1635359715150,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"simSvNCI_l6C","outputId":"3b2a3a8a-70ce-4f69-e594-6c1660b13171"},"outputs":[],"source":["dataset_own.head()"]},{"cell_type":"markdown","id":"f5d0e2ad","metadata":{"id":"f5d0e2ad"},"source":["## Working with several collected data sets (Optional)"]},{"cell_type":"markdown","id":"7269e38b","metadata":{"id":"7269e38b"},"source":["If you want to collect several data sets, you can do that. Then please rerun the above code to create the dataset. Important is of course, that you then adjust the label value, so 0 in case this time you were not active. Also, make sure to change the name of the resulting datafile!"]},{"cell_type":"code","execution_count":null,"id":"137e1a78","metadata":{"id":"137e1a78"},"outputs":[],"source":["class aggregating_datasets:\n","    \n","    def merge_dataset(self, dataset_1, dataset_2):\n","        print('The datasets were included and were merged!')\n","        return pd.concat([dataset_1, dataset_2])\n","        "]},{"cell_type":"code","execution_count":null,"id":"28a71290","metadata":{"id":"28a71290"},"outputs":[],"source":["#Example code how you would merge two datasets. These are called dataset1 and dataset2 here! Uncomment them to make them work\n","#aggregator = aggregating_datasets()\n","#merged_data = aggregator.merge_dataset(dataset1, dataset2)"]},{"cell_type":"markdown","id":"4760bf50","metadata":{"id":"4760bf50"},"source":["## Redo the preprocessing steps "]},{"cell_type":"markdown","id":"315f7525","metadata":{"id":"315f7525"},"source":["Now that you have merged the data, you need to redo the steps for preprocessing. In short, you need to standardization as well as the PCA and the imputing of missing variables again. Also the rolling window function needs to be redone.\n","Of course, if you used only a subset of features due to feaure selection you need to use only that subset as well!"]},{"cell_type":"code","execution_count":null,"id":"5a384ded","metadata":{"id":"5a384ded"},"outputs":[],"source":["#Standardizer\n","\n","data_own_scaled = pd.DataFrame(my_standardizer.transform(dataset_own), columns = col_names)"]},{"cell_type":"code","execution_count":null,"id":"f2e7e873","metadata":{"id":"f2e7e873"},"outputs":[],"source":["#Missing data imputer\n","\n","data_own_new = pd.DataFrame(my_imputer.transform(data_own_scaled), columns = col_names)"]},{"cell_type":"code","execution_count":null,"id":"2e14b909","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1635359731303,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"2e14b909","outputId":"374811e5-503e-4566-ecb5-1e1958add9f9"},"outputs":[],"source":["data_own_new.columns"]},{"cell_type":"code","execution_count":null,"id":"da9aac71","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1635359734196,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"da9aac71","outputId":"1dfa6aea-2e94-45fe-c8c3-76192f16c5c5"},"outputs":[],"source":["#PCA Remember to take the same number of components as before!\n","my_pca.fit_pca(data_own_new)\n","\n","data_own_NEW = my_pca.return_pca_components(1, data_own_new, True, True)"]},{"cell_type":"code","execution_count":null,"id":"6c508edf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1635359737055,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"6c508edf","outputId":"39d5f543-b454-4513-ba69-0ac35c54b474"},"outputs":[],"source":["data_own_NEW.head()"]},{"cell_type":"code","execution_count":null,"id":"59654e91","metadata":{"id":"59654e91"},"outputs":[],"source":["#Rolling window\n","class_roll = rolling_window_aggregator()\n","data_own_mean = class_roll.aggregate(data_own_NEW, 4, 'mean') #Important check here window size!\n","data_own_std = class_roll.aggregate(data_own_NEW, 4, 'std')\n","new1 = class_roll.concat_data(data_own_mean, data_own_std)\n","NEW_dataset = class_roll.concat_data(data_own_NEW, new1)\n"]},{"cell_type":"markdown","id":"bdc3bd5d","metadata":{"id":"bdc3bd5d"},"source":["Now you can finally split the dataset into Xtest and Ytest."]},{"cell_type":"markdown","id":"d90be00b","metadata":{"id":"d90be00b"},"source":["Now we can finally see how good our model performs on our data!"]},{"cell_type":"code","execution_count":null,"id":"580b76bf","metadata":{"id":"580b76bf"},"outputs":[],"source":["#First split the dataset \n","\n","X_test = NEW_dataset.drop(['Active'], axis = 1)\n","Y_test = NEW_dataset[['Active']]"]},{"cell_type":"code","execution_count":null,"id":"32ff9082","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":778,"status":"ok","timestamp":1635359754332,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"32ff9082","outputId":"c0ee08f1-1d58-4c85-969a-e6f14ec5913f"},"outputs":[],"source":["Y_test.head()"]},{"cell_type":"markdown","id":"bebeced3","metadata":{"id":"bebeced3"},"source":["We can make predictions given the models we had before. For example with the ADA Boost we can do the following:"]},{"cell_type":"code","execution_count":null,"id":"3c6f1f50","metadata":{"id":"3c6f1f50"},"outputs":[],"source":["#ADA Boost get the predictions\n","y_pred = vanilla_ada.prediction_ADA(X_test)"]},{"cell_type":"markdown","id":"202c0f16","metadata":{"id":"202c0f16"},"source":["Then, we can get the predictions via:"]},{"cell_type":"code","execution_count":null,"id":"4dfa6370","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":754,"status":"ok","timestamp":1635359762037,"user":{"displayName":"B Ündes","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13583163329665985789"},"user_tz":-120},"id":"4dfa6370","outputId":"5d21616f-ac65-4c79-9aa9-0026a08d0310"},"outputs":[],"source":["print(accuracy_score(y_pred, Y_test))"]},{"cell_type":"markdown","id":"e4b183c7","metadata":{},"source":["---\n","\n","&copy; 2022 - **VU Amsterdam**"]},{"cell_type":"markdown","id":"68d3969e","metadata":{},"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":5}
